# scraper.py
# Se encarga de buscar URLs candidatas (ej. en DuckDuckGo), resolverlas y analizar su metadata.

import json  # Mantenido aunque no se use directamente, estaba en el original
import re  # Mantenido aunque no se use directamente, estaba en el original
from urllib.parse import quote_plus

import analyzer
# Mantener imports necesarios para la bÃºsqueda inicial (DuckDuckGo HTML)
import requests
import web_tools  # Importamos las herramientas web
from bs4 import BeautifulSoup

# Importamos mÃ³dulos de utilidad y anÃ¡lisis
import database


def buscar_noticias(query: str, num_noticias: int = 5, min_score_fuente: int = 5, prompt_analyzer_template: str = None):
    """
    Busca noticias sobre un tema (query), resuelve URLs, analiza con IA y retorna resultados.

    Args:
        query (str): El tema o consulta de bÃºsqueda.
        num_noticias (int): NÃºmero mÃ¡ximo de artÃ­culos a retornar despuÃ©s del filtrado.
        min_score_fuente (int): Score mÃ­nimo que debe tener una fuente analizada para ser considerada.
        prompt_analyzer_template (str, optional): Plantilla de prompt para el analizador. Defaults to None.
    """
    print(f"\nğŸ” Buscando noticias sobre: {query} (Num noticias: {num_noticias}, Score min: {min_score_fuente})")

    # La funciÃ³n interna de bÃºsqueda no necesita cambios.
    def fetch_urls_from_ddg():
        """Realiza la bÃºsqueda en DuckDuckGo y retorna una lista de URLs candidatas."""
        query = f"{tema} site:.es OR site:.com after:2024"
        url = f"https://duckduckgo.com/html/?q={quote_plus(query)}&kl=es-es"
        headers = {'User-Agent': 'Mozilla/5.0'}

        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            return [
                ("https:" + a['href'] if a['href'].startswith("//") else a['href'])
                for a in soup.select('a.result__url')[:10]
                if not any(x in a['href'] for x in ["youtube.com", "facebook.com", "twitter.com", "linkedin.com"])
            ]
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Error en bÃºsqueda de URLs (RequestException): {str(e)}")
            return []
        except Exception as e:
            print(f"âš ï¸ Error en bÃºsqueda de URLs (General Exception): {str(e)}")
            return []

    # --- LÃ³gica principal con la estructura corregida ---
    
    ranked_articles = []
    driver = None  # Inicializar driver fuera del try para que exista en el finally

    try:
        # 1. Iniciar el driver UNA SOLA VEZ.
        try:
            driver = web_tools.setup_driver()
        except Exception as e:
            print(f"âŒ FallÃ³ la configuraciÃ³n del driver de Selenium: {e}. No se podrÃ¡n resolver redirecciones.")
            driver = None # Asegurar que es None si falla

        # 2. Iterar sobre las URLs. El driver ya estÃ¡ vivo.
        for url in fetch_urls_from_ddg():
            try:
                # Resolver redirecciÃ³n (si el driver existe)
                final_url = url
                if driver:
                    resolved = web_tools.get_final_url(url, driver)
                    if resolved:
                        final_url = resolved
                    else:
                        print(f"âš ï¸ Usando URL original por fallo en redirecciÃ³n: {final_url[:60]}...")
                
                # Control de duplicados
                if database.url_existe(final_url):
                    print(f"â© Saltando duplicado: {final_url[:60]}...")
                    continue

                # Filtrar URLs no deseadas
                if any(x in final_url for x in ["/tag/", "/temas/", "?page=", "#", "/category/", ".pdf", ".zip"]):
                    print(f"â© Saltando URL no-articulo/archivo: {final_url[:60]}...")
                    continue

                # Obtener contenido
                text = web_tools.fetch_and_extract_content(final_url)
                if not text:
                    print(f"â© Saltando URL por contenido no extraÃ­do/muy corto: {final_url[:60]}...")
                    continue

                # Analizar con IA
                # TODO: Modificar analyzer.analyze_with_gemini para que acepte prompt_template si se proporciona
                analysis_result = analyzer.analyze_with_gemini(query, text) # Pasando 'query' como el tema para el anÃ¡lisis

                if not analysis_result: # Si el anÃ¡lisis falla o retorna None
                    print(f"â© Saltando URL por fallo en anÃ¡lisis IA: {final_url[:60]}...")
                    continue

                analysis_result['url'] = final_url # Asegurar que la URL estÃ¡ en el resultado
                ranked_articles.append(analysis_result)
                print(f"âœ… Analizado: {final_url[:60]}... | Score: {analysis_result.get('score', 0)}")

            except Exception as e:
                # Capturar error de UNA SOLA URL, para que el bucle continÃºe
                print(f"âš ï¸ Error procesando URL {url}: {e}")
                # continue no es necesario, el bucle avanzarÃ¡ naturalmente

    finally:
        # 3. Cerrar el driver UNA SOLA VEZ al final de todo el proceso.
        if driver:
            try:
                driver.quit()
                print("\nâœ… Driver de Selenium cerrado correctamente.")
            except Exception as e:
                print(f"âš ï¸ Error al cerrar el driver de Selenium: {e}")

    # Filtrar y ordenar los resultados
    # Usar el parÃ¡metro min_score_fuente en lugar del valor hardcodeado 5
    print(f"Filtrando artÃ­culos analizados con score >= {min_score_fuente}...")
    valid_articles = [a for a in ranked_articles if a and isinstance(a.get('score'), (int, float)) and a.get('score', 0) >= min_score_fuente]

    if not valid_articles:
        print(f"â„¹ï¸ No se encontraron artÃ­culos con score >= {min_score_fuente} despuÃ©s del anÃ¡lisis.")
        return []

    valid_articles.sort(key=lambda x: x.get('score', 0), reverse=True)

    print(f"ğŸ† Retornando TOP {min(len(valid_articles), num_noticias)} artÃ­culos de {len(valid_articles)} vÃ¡lidos.")
    return valid_articles[:num_noticias]

