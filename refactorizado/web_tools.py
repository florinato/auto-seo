# web_tools.py
# Contiene funciones de utilidad para interactuar con la web (scraping b√°sico, manejo de URLs, b√∫squeda de im√°genes).

# === Configuraci√≥n de Unsplash API ===
import os

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.support.ui import WebDriverWait
# No importamos EC ya que no se usa en las funciones movidas, aunque estaba en el original.
from webdriver_manager.chrome import ChromeDriverManager

# import urllib.parse # No se usa directamente aqu√≠, se usa en scraper para quote_plus


load_dotenv()

# NOTA IMPORTANTE: Usa variables de entorno para la clave de API de Unsplash.
UNSPLASH_ACCESS_KEY = os.getenv("UNSPLASH_ACCESS_KEY")
UNSPLASH_API_URL = "https://api.unsplash.com/"


def setup_driver():
    """
    Configura y retorna un driver de Selenium optimizado para uso headless.
    Retorna el driver si tiene √©xito, None si falla.
    """
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--log-level=3") # Menos log de Selenium
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--window-size=1920,1080") # Definir tama√±o de ventana para headless

    # Usando Service para compatibilidad con versiones recientes de Selenium y webdriver-manager
    try:
        # Intentar instalar el driver si no est√° presente y obtener su path
        driver_path = ChromeDriverManager().install()
        service = Service(driver_path)
        driver = webdriver.Chrome(service=service, options=options)
        # print("‚úÖ Driver de Selenium configurado correctamente (headless).") # Depuraci√≥n, opcional
        return driver
    except Exception as e:
        print(f"‚ö†Ô∏è Error al configurar el driver de Selenium: {str(e)}")
        print("Aseg√∫rate de tener Chrome instalado y que webdriver-manager funciona correctamente.")
        # No lanzar excepci√≥n, retorna None para que scraper.py pueda continuar sin driver
        return None


def get_final_url(ddg_redirect_url, driver):
    """
    Resuelve redirecciones de DuckDuckGo usando Selenium para obtener la URL final.
    Requiere un driver de Selenium activo.
    Retorna la URL final o None si falla.
    """
    if not driver:
        # print("‚ö†Ô∏è Driver de Selenium no disponible. No se puede resolver redirecci√≥n.") # Depuraci√≥n, opcional
        return None # No se puede resolver sin driver

    try:
        # Navegar a la URL que genera la redirecci√≥n de DDG
        driver.get(ddg_redirect_url)
        # Espera hasta que la URL cambie, indicando la redirecci√≥n ha terminado
        # Aumentar el timeout por si acaso (era 10, ahora 15)
        WebDriverWait(driver, 15).until(
            lambda d: d.current_url != ddg_redirect_url and d.current_url != 'about:blank' and not d.current_url.startswith('https://duckduckgo.com/') # Asegurar que sali√≥ de DDG
        )
        # Asegurarse de que la URL final no es una p√°gina de error o blank
        final_url = driver.current_url
        if final_url and final_url != 'about:blank' and not final_url.startswith('data:'):
            # print(f"‚û°Ô∏è Redirecci√≥n resuelta a: {final_url[:60]}...") # Depuraci√≥n, opcional
            return final_url
        else:
             print(f"‚ö†Ô∏è Redirecci√≥n a URL inesperada o en blanco para {ddg_redirect_url[:60]}...")
             return None # No se redirigi√≥ a una URL √∫til
    except Exception as e:
        # Capturamos Timeouts u otros errores durante la redirecci√≥n
        print(f"‚ö†Ô∏è Error de Selenium en redirecci√≥n para {ddg_redirect_url[:60]}...: {str(e)}")
        return None # Fall√≥ la resoluci√≥n de la URL


def extract_article_content(soup):
    """
    Extrae el contenido principal del art√≠culo de un objeto BeautifulSoup.
    Intenta identificar bloques de contenido comunes y limpia elementos irrelevantes.
    Retorna el texto extra√≠do o una cadena vac√≠a/None si no se encuentra contenido significativo.
    """
    # Remover elementos que generalmente no son parte del cuerpo del art√≠culo
    for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'aside', 'header', 'form', '.sidebar']): # A√±adidos m√°s selectores comunes
        if element: # Added check
            try:
                element.decompose()
            except Exception as e:
                 # print(f"‚ö†Ô∏è Error al descomponer elemento: {e}") # Depuraci√≥n, opcional
                 pass


    # Selectores comunes para identificar el cuerpo del art√≠culo
    selectors = [
        'article', # La etiqueta sem√°ntica article
        '.article-content', '.entry-content', '.post-content', # Clases comunes
        '#main-content', '#content', # IDs comunes para el √°rea principal
        'div[itemprop="articleBody"]', # Microdatos
        'div.body', 'div.story', 'div.text', 'div.content' # Otros selectores gen√©ricos
    ]

    content_element = None
    for selector in selectors:
        content_element = soup.select_one(selector)
        if content_element:
            # print(f"‚úÖ Contenido encontrado usando selector: {selector}") # Depuraci√≥n, opcional
            break # Encontramos un candidato, salimos del bucle

    # Si no se encontr√≥ un contenedor espec√≠fico, intentar con el body o un div gen√©rico
    if not content_element:
        # print("‚ö†Ô∏è No se encontr√≥ contenedor espec√≠fico. Intentando fallback con body o div.") # Depuraci√≥n, opcional
        content_element = soup.body or soup.find('div') # Fallback al body o al primer div

    if not content_element:
         # print("‚ùå No se encontr√≥ ning√∫n elemento para extraer contenido.") # Depuraci√≥n, opcional
         return None


    # Extraer texto de los p√°rrafos dentro del elemento encontrado
    # Limite de p√°rrafos para evitar descargar/procesar contenido masivo accidentalmente
    paragraphs = content_element.find_all('p', limit=20) # Aumentado ligeramente el l√≠mite

    # Si no hay p√°rrafos directos, intentar extraer texto de otros elementos de bloque comunes
    if not paragraphs:
        # print("‚ö†Ô∏è No se encontraron <p> directos. Intentando otros elementos de bloque.") # Depuraci√≥n, opcional
        block_elements = content_element.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'li', 'blockquote', 'pre', 'div'], limit=20) # A√±adir otros elementos relevantes
        if block_elements:
             # Concatenar el texto de los elementos encontrados, a√±adiendo saltos de l√≠nea para simular estructura
             content_text = "\n\n".join(elem.get_text(strip=True) for elem in block_elements)
             if len(content_text) > 100: # M√≠nimo de caracteres para considerar que hay contenido
                  # print(f"‚úÖ Contenido extra√≠do de elementos de bloque: {len(content_text)} chars") # Depuraci√≥n, opcional
                  return content_text
             else:
                  # print("‚ö†Ô∏è Contenido extra√≠do de elementos de bloque muy corto.") # Depuraci√≥n, opcional
                  return None

    # Si se encontraron p√°rrafos, unirlos
    if paragraphs:
        content_text = ' '.join(p.get_text(strip=True) for p in paragraphs) # Usar strip=True para limpiar espacios en blanco
        if len(content_text) > 100:
            # print(f"‚úÖ Contenido extra√≠do de <p>: {len(content_text)} chars") # Depuraci√≥n, opcional
            return content_text
        else:
             # print("‚ö†Ô∏è Contenido extra√≠do de <p> muy corto.") # Depuraci√≥n, opcional
             return None

    # Si no se encontr√≥ contenido significativo por ninguno de los m√©todos
    # print("‚ùå No se pudo extraer contenido significativo.") # Depuraci√≥n, opcional
    return None


def fetch_and_extract_content(url, timeout=15):
    """
    Descarga el HTML de una URL y extrae el contenido principal del art√≠culo.
    Funci√≥n de conveniencia para usar en otros m√≥dulos.
    Retorna el texto extra√≠do o None si falla o el contenido es insuficiente.
    """
    try:
        # Headers m√°s amigables
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'} # User-Agent m√°s com√∫n

        response = requests.get(url, headers=headers, timeout=timeout)
        response.raise_for_status() # Lanza excepci√≥n para errores HTTP (4xx, 5xx)

        # Usar response.content y especificar encoding si es posible
        soup = BeautifulSoup(response.content, 'html.parser')

        # Usamos la funci√≥n de extracci√≥n mejorada
        content = extract_article_content(soup)

        # Umbral m√≠nimo de texto extra√≠do y verificaci√≥n de contenido no deseado (ej: mensajes de cookie)
        if not content or len(content) < 200 or "aceptar cookies" in content.lower() or "suscribete" in content.lower()[:200]: # Aumentado umbral, a√±adido filtro b√°sico
             # print(f"‚ö†Ô∏è Contenido extra√≠do muy corto ({len(content) if content else 0} chars) o sospechoso de {url[:60]}...") # Depuraci√≥n, opcional
             return None

        # Limpieza final de texto (ej: eliminar espacios excesivos)
        content = ' '.join(content.split()).strip() # Reemplazar m√∫ltiples espacios/saltos por uno solo

        # print(f"‚úÖ Contenido extra√≠do y limpio de {url[:60]}... ({len(content)} chars)") # Depuraci√≥n, opcional
        return content

    except requests.exceptions.RequestException as e:
        # print(f"‚ö†Ô∏è Error de red o HTTP al descargar {url[:60]}...: {str(e)}") # Depuraci√≥n, opcional
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Error general al descargar y extraer de {url[:60]}...: {str(e)}")
        return None


# === NUEVA FUNCI√ìN: B√∫squeda de Im√°genes en Unsplash ===
def find_free_images(query, num_results=3):
    """
    Busca im√°genes en Unsplash API basadas en una query.

    Args:
        query (str): T√©rmino de b√∫squeda (ej: tema del art√≠culo, tags).
        num_results (int): N√∫mero m√°ximo de resultados a obtener (max 30 por p√°gina en Unsplash).

    Returns:
        list: Lista de diccionarios con metadata de imagen, o lista vac√≠a si falla.
              Ej: [{'url': '...', 'alt_text': '...', 'author': '...', 'license': '...', 'author_url': '...', 'source_page_url': '...'}]
    """
    if not UNSPLASH_ACCESS_KEY or UNSPLASH_ACCESS_KEY == "TU_UNSPLASH_ACCESS_KEY":
        print("‚ùå Error: UNSPLASH_ACCESS_KEY no configurada. No se puede buscar im√°genes.")
        return []

    search_url = f"{UNSPLASH_API_URL}search/photos"
    headers = {
        "Authorization": f"Client-ID {UNSPLASH_ACCESS_KEY}",
        "Accept-Version": "v1" # Requerido por la API de Unsplash
    }
    params = {
        "query": query,
        "per_page": min(num_results, 30), # Unsplash API tiene un l√≠mite de 30 por p√°gina
        "orientation": "landscape", # Buen formato para blogs
        "content_filter": "high", # O "low", dependiendo de la tolerancia a contenido sensible
        "lang": "es" # Si la API soporta lenguaje para la b√∫squeda
    }

    print(f"\nüì∏ Buscando im√°genes en Unsplash para: '{query}'...")

    response = None # Inicializar response a None

    try:
        response = requests.get(search_url, headers=headers, params=params, timeout=10)
        response.raise_for_status() # Lanza excepci√≥n para errores HTTP (4xx, 5xx)

        data = response.json()

        if not data or 'results' not in data or not data['results']:
            print(f"‚ö†Ô∏è B√∫squeda de im√°genes para '{query}' no retorn√≥ resultados.")
            return []

        images_metadata = []
        for item in data['results']:
            # Extraer info relevante. Los campos pueden variar, revisar docs de Unsplash API.
            image_info = {
                # Usamos 'regular' o 'small' size para previews. 'full' o 'raw' son demasiado grandes.
                'url': item['urls'].get('regular') or item['urls'].get('small'),
                # Alt text: intentar descripci√≥n, alt_description, o un default
                'alt_text': item.get('alt_description') or item.get('description') or f"Imagen sobre {query}",
                # Caption: puede ser similar al alt_text o m√°s detallado si existe
                'caption': item.get('description') or item.get('alt_description'),
                'author': item['user'].get('name', 'Unsplash'),
                'author_url': item['user'].get('links', {}).get('html'), # URL del perfil del autor para atribuci√≥n
                'source_page_url': item['links'].get('html'), # URL de la p√°gina de la imagen en Unsplash (para atribuci√≥n)
                'license': 'Unsplash License' # Generalmente todas son as√≠ en la API p√∫blica
            }
            # Filtrar si la URL de la imagen no existe o es None
            if image_info['url']:
                 images_metadata.append(image_info)
                 # print(f"   - Encontrada imagen: {image_info['url'][:60]}...") # Depuraci√≥n, opcional

        print(f"‚úÖ Encontradas {len(images_metadata)} im√°genes para '{query}'.")
        return images_metadata

    except requests.exceptions.RequestException as e:
        print(f"‚ùå Error de red o HTTP al buscar im√°genes en Unsplash: {str(e)}")
        print(f"   Detalles: URL={search_url}, Query='{query}'")
        if response is not None:
             print(f"   Status Code: {response.status_code}")
             try: # Intenta imprimir el cuerpo del error si es JSON
                 error_json = response.json()
                 import json
                 print(f"   Error body (JSON): {json.dumps(error_json, indent=2)[:500]}...")
             except json.JSONDecodeError: # Si no es JSON, imprime texto
                 error_body = response.text
                 if len(error_body) < 500:
                      print(f"   Response body (text): {error_body}")
                 else:
                      print(f"   Response body (text, first 500 chars): {error_body[:500]}...")
             except Exception as inner_e:
                 print(f"   Error imprimiendo cuerpo de respuesta: {inner_e}")
        return []
    except Exception as e:
        print(f"‚ùå Error general al buscar im√°genes en Unsplash: {str(e)}")
        return []


# Bloque __main__ para pruebas independientes (opcional)
if __name__ == "__main__":
    print("--- Probando web_tools.py ---")

    # --- Prueba de B√∫squeda y Extracci√≥n de Contenido ---
    test_urls = [
        "https://www.example.com/article-about-technology", # Reemplaza con URL de prueba real si tienes
        "https://www.bbc.com/news/world-europe-67890123", # Ejemplo de URL real
        "https://www.nytimes.com/2024/01/01/technology/ai-advances.html" # Ejemplo de URL real
    ]

    print("\n--- Probando fetch_and_extract_content ---")
    # Puedes descomentar esto y poner URLs reales para probar
    # for url in test_urls:
    #     print(f"\nIntentando descargar y extraer: {url}")
    #     content = fetch_and_extract_content(url)
    #     if content:
    #         print(f"Contenido extra√≠do (primeros 500 chars):\n{content[:500]}...")
    #     else:
    #         print("Fallo la extracci√≥n.")

    # --- Prueba de B√∫squeda de Im√°genes ---
    print("\n--- Probando find_free_images (Requiere UNSPLASH_ACCESS_KEY) ---")
    test_query_image = "Inteligencia Artificial en Medicina"
    if UNSPLASH_ACCESS_KEY != "TU_UNSPLASH_ACCESS_KEY":
        images = find_free_images(test_query_image, num_results=2)
        if images:
            print(f"\nResultados de im√°genes para '{test_query_image}':")
            for i, img in enumerate(images):
                print(f"  Imagen {i+1}:")
                print(f"    URL: {img.get('url')[:80]}...")
                print(f"    Alt Text: {img.get('alt_text')}")
                print(f"    Autor: {img.get('author')}")
                print(f"    Licencia: {img.get('license')}")
                print(f"    URL Autor: {img.get('author_url')}")
                print(f"    URL P√°gina: {img.get('source_page_url')}")
        else:
            print(f"No se encontraron im√°genes para '{test_query_image}'.")
    else:
        print("Saltando prueba de im√°genes: UNSPLASH_ACCESS_KEY no configurada.")


    # --- Prueba de Selenium Driver y Redirecci√≥n (Opcional, requiere Chrome instalado) ---
    # print("\n--- Probando setup_driver y get_final_url ---")
    # try:
    #     driver = setup_driver()
    #     if driver:
    #         ddg_url_redirect_example = "https://duckduckgo.com/r/?context=images&k=1&uddg=https%3A%2F%2Fwww.example.com%2Fsome-image-page" # Ejemplo de URL de redirecci√≥n de DDG
    #         print(f"\nIntentando resolver redirecci√≥n: {ddg_url_redirect_example}")
    #         final_url_resolved = get_final_url(ddg_url_redirect_example, driver)
    #         if final_url_resolved:
    #             print(f"URL Resuelta: {final_url_resolved}")
    #         else:
    #             print("Fallo la resoluci√≥n de la URL.")
    #     else:
    #         print("Fallo la configuraci√≥n del driver.")
    # except Exception as e:
    #      print(f"Error en la prueba del driver: {e}")
    # finally:
    #     if driver:
    #         driver.quit()
    #         print("\nDriver de Selenium cerrado.")


    print("\n--- Fin de la prueba de web_tools.py ---")
